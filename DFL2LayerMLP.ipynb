{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from UtilityGraph import *\n",
    "from Defence import *\n",
    "from Corruption import *\n",
    "from UtilityMLP import *\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "from Test import *\n",
    "from ipywidgets import interact, IntSlider\n",
    "import copy\n",
    "\n",
    "# Device configuration (use GPU if available)\n",
    "device = torch.device('cuda')\n",
    "# %matplotlib notebook\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seed = 42  # for reproducibility\n",
    "random.seed(Seed)\n",
    "np.random.seed(Seed) \n",
    "torch.manual_seed(Seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(Seed)\n",
    "    torch.cuda.manual_seed_all(Seed)\n",
    "\n",
    "# Graph\n",
    "required_probability=0.9999\n",
    "Amount_Clients = 20\n",
    "num_nodes, G,A,pos,r_c=build_random_graph(Amount_Clients,required_probability,fix_num_nodes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"num_nodes:\",num_nodes)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28        # MNIST images are 28x28 pixels\n",
    "hidden_size = 128           # Number of neurons in the hidden layer\n",
    "num_classes = 10            # Number of output classes (digits 0-9)\n",
    "num_epochs = 5             # Number of local training epochs per aggregation \n",
    "batch_size = 64             # Batch size for training\n",
    "learning_rate = 0.01        # Learning rate for the optimizer\n",
    "num_clients = num_nodes     # Number of clients \n",
    "num_rounds = 2              # Number of aggregation rounds\n",
    "threshold = 0.02            # Loss threshold for stopping criteria\n",
    "\n",
    "percentageCorrupt = 1/num_nodes        #Percentage of corrupt clients\n",
    "corrupt = True              #If True, corrupt clients are included\n",
    "CorruptClients = CorruptGeneration(percentageCorrupt, corrupt, num_clients) #Selection of Corrupt Clients\n",
    "\n",
    "lying_nodes = 0\n",
    "lying_nodes = np.where(CorruptClients == 1)[0]\n",
    "true_nodes = [i for i in range(num_nodes) if i not in lying_nodes]\n",
    "\n",
    "all_nodes = np.union1d(lying_nodes, true_nodes)\n",
    "print(\"Corrupt Clients:\", lying_nodes)\n",
    "tolerance=-1                #PDMM tolerance\n",
    "c=0.5                       #PDMM c\n",
    "max_iters=300             #PDMM max iterations\n",
    "when = 0\n",
    "CorruptValue = -100000000000000000\n",
    "\n",
    "rejection_threshold = 20\n",
    "K_decision = 5\n",
    "averaging = 1\n",
    "# noise_levels = [0]\n",
    "# noise_levels = [0, 1] \n",
    "# noise_levels = [0, 10**-2,1,10**2,10**4] \n",
    "# noise_levels = [0, 10**-6,10**-4,10**-2,1] \n",
    "noise_levels = [10**4, 10**8, 10**16] \n",
    "\n",
    "var = 10                  # Standard deviation for Gaussian noise\n",
    "mean = 10**8                   # Mean for Gaussian noise\n",
    "Target = np.random.randint(1, num_clients) # Target client for copycat attack\n",
    "scale = 0\n",
    "typeAttack = 1              # 0: Label Flipping attack, 1: Gaussian noise, 2: Copycat attack, 3: Gaussian addative noise attack, 4: LIE attack, 5 Sign flip attack, 6: No attack\n",
    "PrivacyMethod = 3       # 0: No privacy, 1: DP, 2: SMPC, 3: Subspace\n",
    "# PrimModulo = [2**32 - 5, 2**32 - 5, 2**61 - 1] # Prime number for SMPC should be above sum of all client local model values\n",
    "PrimModulo = [0]\n",
    "\n",
    "neighbors_dict = {}\n",
    "for ln in lying_nodes:\n",
    "    neighbors_dict[ln] = list(G.neighbors(ln))\n",
    "save = False\n",
    "print(\"Neighbors of lying nodes:\", neighbors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "color_map = []\n",
    "for node in range(num_nodes):\n",
    "    if node in lying_nodes:\n",
    "        color_map.append('red')   # corrupt\n",
    "    else:\n",
    "        color_map.append('blue')  # honest\n",
    "\n",
    "nx.draw(G, pos, with_labels=True, node_color=color_map)\n",
    "plt.title(\"Graph with Lying (Red) and Honest (Blue) Nodes\")\n",
    "plt.show()\n",
    "\n",
    "# Create a subgraph containing only honest nodes\n",
    "remaining_nodes = [n for n in G.nodes() if n not in lying_nodes]\n",
    "G_sub = G.subgraph(remaining_nodes)\n",
    "\n",
    "still_connected = nx.is_connected(G_sub)\n",
    "print(\"Are the honest‐only nodes still forming a connected subgraph?\", still_connected)\n",
    "\n",
    "# Check that every honest node has a majority of honest neighbors\n",
    "all_honest = [n for n in G.nodes() if n not in lying_nodes]\n",
    "all_good = True  # Flag to track if all honest nodes pass the check\n",
    "\n",
    "for node in all_honest:\n",
    "    neighbors = list(G.neighbors(node))\n",
    "    total_neighbors = len(neighbors)\n",
    "    # Count honest neighbors (neighbors that are not in lying_nodes)\n",
    "    honest_neighbors = sum(1 for neighbor in neighbors if neighbor not in lying_nodes)\n",
    "    \n",
    "    # Only check nodes that have at least one neighbor; if a node is isolated, it might need special handling\n",
    "    if total_neighbors > 0:\n",
    "        if honest_neighbors <= total_neighbors / 2:\n",
    "            print(f\"Honest node {node} does not have majority honest neighbors: {honest_neighbors} honest out of {total_neighbors} neighbors\")\n",
    "            all_good = False\n",
    "            \n",
    "print(\"Are all honest nodes connected to a majority of honest neighbors?\", all_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_metrics(results, round, typeAttack, PrivacyMethod,save):\n",
    "    # Define markers for different sigma values\n",
    "    markers = ['o', 's', 'd', '^', 'v', '<', '>', 'p', '*', 'h', '+', 'x']\n",
    "    \n",
    "    # Group results by noise level\n",
    "    noise_groups = {}\n",
    "    for res in results:\n",
    "        if PrivacyMethod == 2:\n",
    "            key = 0\n",
    "        else:\n",
    "            # use .get so missing keys map to 0\n",
    "            key = res.get('noise_STD', 0) or 0\n",
    "        noise_groups.setdefault(key, []).append(res)\n",
    "\n",
    "    # Create plots\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 8))\n",
    "\n",
    "    # Convert noise levels to sorted list for consistent markers\n",
    "    sorted_noises = sorted(noise_groups.keys())\n",
    "\n",
    "    # Define base sparsity (every N points)\n",
    "    base_markevery = 17  \n",
    "    \n",
    "    # Plot FAR\n",
    "    for idx, noise in enumerate(sorted_noises):\n",
    "        group = noise_groups[noise]\n",
    "        far_matrix = np.array([res['FAR'] for res in group])\n",
    "        avg_far = np.mean(far_matrix, axis=0)\n",
    "        marker = markers[idx % len(markers)]  # Cycle through markers\n",
    "        offset = (idx*50) % base_markevery  # Offset to stagger marker placement\n",
    "        markevery = (offset, base_markevery)  # Staggered marker appearance\n",
    "        if PrivacyMethod != 2:\n",
    "            if noise != 0:\n",
    "                axs[0].plot(avg_far, marker=marker, markevery=markevery, label=f'$\\\\sigma^2 = 10^{{{int(np.log10(noise))}}}$', markersize=6)\n",
    "            else:\n",
    "                axs[0].plot(avg_far, marker=marker, markevery=markevery, label='$\\\\sigma^2 = 0$', markersize=6)\n",
    "        if PrivacyMethod == 2:\n",
    "            axs[0].plot(avg_far, marker=marker, markevery=markevery, markersize=6)\n",
    "    axs[0].set_title(f'False Alarm Rate (Round {round})')\n",
    "    axs[0].set_ylabel('FAR')\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Plot MDR\n",
    "    for idx, noise in enumerate(sorted_noises):\n",
    "        group = noise_groups[noise]\n",
    "        mdr_matrix = np.array([res['MDR'] for res in group])\n",
    "        avg_mdr = np.mean(mdr_matrix, axis=0)\n",
    "        marker = markers[idx % len(markers)]\n",
    "        offset = (idx*50) % base_markevery\n",
    "        markevery = (offset, base_markevery)\n",
    "        if PrivacyMethod != 2:\n",
    "            if noise != 0:\n",
    "                axs[1].plot(avg_mdr, marker=marker, markevery=markevery, label=f'$\\\\sigma^2 = 10^{{{int(np.log10(noise))}}}$', markersize=6)\n",
    "            else:\n",
    "                axs[1].plot(avg_mdr, marker=marker, markevery=markevery, label='$\\\\sigma^2 = 0$', markersize=6)\n",
    "        if PrivacyMethod == 2:\n",
    "                axs[1].plot(avg_mdr, marker=marker, markevery=markevery , markersize=6)\n",
    "    axs[1].set_title(f'Missed Detection Rate (Round {round})')\n",
    "    axs[1].set_ylabel('MDR')\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # Plot Error\n",
    "    for idx, noise in enumerate(sorted_noises):\n",
    "        group = noise_groups[noise]\n",
    "        err_matrix = np.array([res['Error'] for res in group])\n",
    "        avg_err = np.mean(err_matrix, axis=0)\n",
    "        marker = markers[idx % len(markers)]\n",
    "        offset = (idx*50) % base_markevery\n",
    "        markevery = (offset, base_markevery)\n",
    "        if PrivacyMethod != 2:\n",
    "            if noise != 0:\n",
    "                axs[2].plot(avg_err, marker=marker, markevery=markevery, label=f'$\\\\sigma^2 = 10^{{{int(np.log10(noise))}}}$', markersize=6)\n",
    "            else:\n",
    "                axs[2].plot(avg_err, marker=marker, markevery=markevery, label='$\\\\sigma^2 = 0$', markersize=6)\n",
    "        if PrivacyMethod == 2:\n",
    "            axs[2].plot(avg_err, marker=marker, markevery=markevery, markersize=6)\n",
    "    axs[2].set_title(f'Consensus Error (Round {round})')\n",
    "    axs[2].set_ylabel('Error')\n",
    "    axs[2].set_xlabel('Iteration')\n",
    "    axs[2].set_yscale('log')\n",
    "    axs[2].grid(True)\n",
    "    if PrivacyMethod != 2:\n",
    "        axs[2].legend()  # Add legend for the third plot\n",
    "    plt.tight_layout()\n",
    "    if save == True:\n",
    "        filename = f\"combined_metrics_typeAttack{typeAttack}_PrivacyMethod{PrivacyMethod}_Round{round}.png\"\n",
    "        plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the -layer MLP model\n",
    "class Current(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Current, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer\n",
    "        self.relu = nn.ReLU()                          # ReLU activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # Second fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, input_size)  # Flatten the input tensor\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Current(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_classes):\n",
    "#         super(Current, self).__init__()\n",
    "        \n",
    "#         self.layers = nn.Sequential(\n",
    "#             nn.Linear(input_size, hidden_size),  # Layer 1\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_size, hidden_size), # Layer 2\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_size, hidden_size), # Layer 3\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_size, hidden_size), # Layer 4\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_size, hidden_size), # Layer 5\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_size, hidden_size), # Layer 6\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_size, num_classes)  # Output Layer (Layer 7)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the input\n",
    "#         return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Current(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_classes):\n",
    "#         super(Current, self).__init__()\n",
    "#         # First convolution: input 1 channel, output 32 channels, 3x3 kernel, stride 1\n",
    "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1)\n",
    "#         # Second convolution: input 32 channels, output 64 channels, 3x3 kernel, stride 1\n",
    "#         self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
    "#         # Max pooling with a 2x2 kernel (default stride is kernel_size)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "#         # Dropout before flattening with p=0.25\n",
    "#         self.dropout1 = nn.Dropout(p=0.25)\n",
    "#         # Fully connected layer: input dimension is 64 channels * 12 * 12 = 9216, output is 128\n",
    "#         self.fc1 = nn.Linear(in_features=64 * 12 * 12, out_features=128)\n",
    "#         # Dropout after first FC with p=0.5\n",
    "#         self.dropout2 = nn.Dropout(p=0.5)\n",
    "#         # Final fully connected layer: 128 -> 10 classes\n",
    "#         self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x is expected to be of shape (batch_size, 1, 28, 28)\n",
    "#         # First conv + ReLU: output shape (batch_size, 32, 26, 26)\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         # Second conv + ReLU: output shape (batch_size, 64, 24, 24)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         # Max pooling: output shape (batch_size, 64, 12, 12)\n",
    "#         x = self.pool(x)\n",
    "#         # Apply dropout then flatten the tensor to (batch_size, 9216)\n",
    "#         x = self.dropout1(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         # First fully connected layer + ReLU: output shape (batch_size, 128)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         # Apply second dropout\n",
    "#         x = self.dropout2(x)\n",
    "#         # Final fully connected layer: output shape (batch_size, 10)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, num_clients):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = np.random.permutation(dataset_size)  # Shuffle all indices\n",
    "    data_per_client = dataset_size // num_clients\n",
    "    split_sizes = [data_per_client] * num_clients\n",
    "\n",
    "    # Distribute any remaining data among the first few clients\n",
    "    for i in range(dataset_size % num_clients):\n",
    "        split_sizes[i] += 1\n",
    "\n",
    "    datasets = []\n",
    "    start = 0\n",
    "    for size in split_sizes:\n",
    "        datasets.append(Subset(dataset, indices[start:start + size]))\n",
    "        start += size  # Move the start index forward\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(dataset, global_model, num_epochs):\n",
    "    \"\"\"Train a local model initialized with global weights\"\"\"\n",
    "    model =  type(global_model)(input_size, hidden_size, num_classes).to(device)\n",
    "    model.load_state_dict(global_model.state_dict()) \n",
    "    model.train()\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "        #params = list(model.parameters())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_models2(local_models, input_size, hidden_size, num_classes, num_nodesSelected):\n",
    "    \"\"\"\n",
    "    Compute the averaged model parameters from local models and return a new model.\n",
    "\n",
    "    Args:\n",
    "        local_models (list): List of PyTorch models (local models).\n",
    "        input_size (int): Input size for the model.\n",
    "        hidden_size (int): Hidden layer size.\n",
    "        num_classes (int): Number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: A new model with averaged parameters.\n",
    "    \"\"\"\n",
    "    # Extract state_dicts from local models\n",
    "    local_dicts = [model.state_dict() for model in local_models]\n",
    "\n",
    "    # Initialize global_dict with zero tensors of the same shape as the first model's parameters\n",
    "    global_dict = {key: torch.zeros_like(local_dicts[0][key]) for key in local_dicts[0].keys()}\n",
    "\n",
    "    # Compute the average over all local models\n",
    "    for key in global_dict.keys():\n",
    "        global_dict[key] = sum(local_dicts[i][key] for i in num_nodesSelected) / len(num_nodesSelected)\n",
    "\n",
    "    averaged_model = type(local_models[0])(input_size, hidden_size, num_classes).to(device)  \n",
    "    averaged_model.load_state_dict(global_dict)\n",
    "\n",
    "    return averaged_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_models(True_avg, node_models, G, tolerance, c, max_iters, rejection_threshold, K_decision, averaging, when, CorruptValue, true_nodes, Print_Val, noise_STD, PrivacyMethod, p, learning_rate , perm_threshold=0.5):\n",
    "    # Initialize variables\n",
    "    num_nodes = len(node_models)\n",
    "    converged = False\n",
    "    count = 0\n",
    "    Error = []\n",
    "    Track = 0\n",
    "    mask_history = []\n",
    "    # lying_nodes = lying_nodes or set()\n",
    "    nodes = list(G.nodes())\n",
    "    True_avg_dict = True_avg.state_dict()\n",
    "\n",
    "    # Detection \n",
    "    mask = np.ones((num_nodes, num_nodes), dtype=int)  # 1: active, -1: blocked\n",
    "    reject_count = np.zeros((num_nodes, num_nodes), dtype=int)\n",
    "    sum_check = np.zeros((num_nodes, num_nodes))\n",
    "    thresholds = np.zeros(num_nodes)\n",
    "    MAD = np.zeros(num_nodes)\n",
    "    D = {i: {j: 0 for j in G.neighbors(i)} for i in G.nodes()}  # Suspicion scores\n",
    "    ignored = {i: set() for i in G.nodes()}  # Ignored neighbors\n",
    "\n",
    "    # Model parameter initialization\n",
    "    local_dicts = [model.state_dict() for model in node_models]\n",
    "    # ------------------------------------------------------------------\n",
    "    # grab current gradients; use zeros if none are present\n",
    "    grad_dicts = []\n",
    "    for model in node_models:\n",
    "        g_node = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            g_node[name] = (param.grad.detach().clone()\n",
    "                            if param.grad is not None\n",
    "                            else torch.zeros_like(param.data))\n",
    "        grad_dicts.append(g_node)\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    global_dict = node_models[0].state_dict() \n",
    "    # param_keys = True_avg.keys()  # all models have the same parameters\n",
    "    A_ij = calc_incidence_nested(G)\n",
    "    x_history = []\n",
    "    # Initialize PDMM variables with tensor support\n",
    "    x = [{} for _ in range(num_nodes)]\n",
    "    z = [{} for _ in range(num_nodes)]\n",
    "    y = [{} for _ in range(num_nodes)]\n",
    "    y_transmit = [{} for _ in range(num_nodes)]\n",
    "    # Initialize x with local models and move to device\n",
    "    for i in range(num_nodes):\n",
    "        for key in True_avg_dict.keys():\n",
    "            # if PrivacyMethod == 1:\n",
    "            #     x[i][key] = local_dicts[i][key].clone() + torch.randn_like(local_dicts[i][key]) * noise_STD\n",
    "            # else:\n",
    "                x[i][key] = local_dicts[i][key].clone()\n",
    "\n",
    "    # Initialize z and y\n",
    "    for i in range(num_nodes):\n",
    "        z[i] = {}\n",
    "        y[i] = {}\n",
    "        y_transmit[i] = {}\n",
    "        for j in G.neighbors(i):\n",
    "            z[i][j] = {}\n",
    "            y[i][j] = {}\n",
    "            y_transmit[i][j] = {}\n",
    "            for key in True_avg_dict.keys():\n",
    "                if PrivacyMethod == 3:\n",
    "                    z[i][j][key] = torch.randn_like(True_avg_dict[key]) * noise_STD\n",
    "                else:\n",
    "                    z[i][j][key] = torch.zeros_like(True_avg_dict[key])\n",
    "                y[i][j][key] = torch.zeros_like(True_avg_dict[key])\n",
    "                y_transmit[i][j][key] = torch.zeros_like(True_avg_dict[key])\n",
    "         \n",
    "    if PrivacyMethod == 2:\n",
    "        smpc_masks = {}\n",
    "        for i in range(num_nodes):\n",
    "            for j in G.neighbors(i):\n",
    "                if i < j:\n",
    "                    smpc_masks[(i, j)] = {}\n",
    "                    for key in True_avg_dict:\n",
    "                        smpc_masks[(i, j)][key] = torch.randn_like(True_avg_dict[key])\n",
    "\n",
    "    # print(lying_nodes)\n",
    "    # Synchronous PDMM with detection\n",
    "    while not converged and count < max_iters:\n",
    "        # --------------------\n",
    "        # 1. Synchronous x-update for all nodes\n",
    "        # --------------------\n",
    "        x_new = [{} for _ in range(num_nodes)]\n",
    "        for i in range(num_nodes):\n",
    "            # Count corrupt neighbors once for node i\n",
    "            corrupt_neighbors = sum(1 for j in G.neighbors(i) if mask[i][j] == -1)\n",
    "            effective_degree = G.degree[i] - corrupt_neighbors\n",
    "\n",
    "            # Update each parameter using the same effective degree\n",
    "            for key in True_avg_dict:\n",
    "                x_new[i][key] = local_dicts[i][key] - learning_rate * grad_dicts[i][key]\n",
    "                for j in G.neighbors(i):\n",
    "                    if mask[i][j] != -1:\n",
    "                        x_new[i][key] -= A_ij[i][j] * z[i][j][key]\n",
    "                x_new[i][key] = x_new[i][key] / (1 + c * effective_degree)\n",
    "\n",
    "        x = x_new\n",
    "        x_history.append(x.copy())\n",
    "\n",
    "        # --------------------\n",
    "        # 2. Dual variable update (y)\n",
    "        # --------------------\n",
    "        for i in range(num_nodes):\n",
    "            for j in G.neighbors(i):\n",
    "                for key in True_avg_dict:\n",
    "                    y[i][j][key] = z[i][j][key] + 2 * c * A_ij[i][j] * x[i][key]\n",
    "                    if PrivacyMethod == 1:\n",
    "                        y[i][j][key] = y[i][j][key].clone() + torch.randn_like(local_dicts[i][key]) * noise_STD\n",
    "\n",
    "        if PrivacyMethod == 2:\n",
    "            # SMPC: Apply pairwise masks for secure aggregation\n",
    "            for i in range(num_nodes):\n",
    "                for j in G.neighbors(i):\n",
    "                    if mask[i][j] == -1:\n",
    "                        for key in True_avg_dict:\n",
    "                            y_transmit[i][j][key] = torch.zeros_like(y[i][j][key])\n",
    "                    else:\n",
    "                        for key in True_avg_dict:\n",
    "                            if i < j:\n",
    "                                y_transmit[i][j][key] = y[i][j][key] + smpc_masks[(i, j)][key]\n",
    "                            else:\n",
    "                                y_transmit[i][j][key] = y[i][j][key] - smpc_masks[(j, i)][key]\n",
    "        else:\n",
    "            # Original: no additional masking\n",
    "            for i in range(num_nodes):\n",
    "                for j in G.neighbors(i):\n",
    "                    if mask[i][j] == -1:\n",
    "                        for key in True_avg_dict:\n",
    "                            y_transmit[i][j][key] = torch.zeros_like(y[i][j][key])\n",
    "                    else:\n",
    "                        for key in True_avg_dict:\n",
    "                            y_transmit[i][j][key] = y[i][j][key].clone()\n",
    "\n",
    "\n",
    "        # --------------------\n",
    "        # 3. Detection logic (executed periodically)\n",
    "        # --------------------\n",
    "        # if count > when:\n",
    "        print(f\"Detecting at count {count}\")\n",
    "        for i in range(num_nodes):\n",
    "            neighbors = [j for j in G.neighbors(i) if j not in ignored[i]]\n",
    "            if not neighbors:\n",
    "                continue\n",
    "\n",
    "            # Precompute absolute values of y variables (for PDMM minus-sign handling)\n",
    "            abs_y = {j: {key: torch.abs(y_transmit[j][i][key]) for key in True_avg_dict} for j in neighbors}\n",
    "\n",
    "            # 1. Compute element-wise median (m_i)\n",
    "            medians = {}\n",
    "            for key in True_avg_dict:\n",
    "                # Stack all neighbors' parameters for this key\n",
    "                params = torch.stack([abs_y[j][key] for j in neighbors])\n",
    "                medians[key] = torch.median(params, dim=0).values  # Element-wise median\n",
    "\n",
    "            # 2. Compute Delta Y_{i,j} using infinity norm\n",
    "            delta_ys = []\n",
    "            for j in neighbors:\n",
    "                max_diff = -float('inf')\n",
    "                for key in True_avg_dict:\n",
    "                    diff = torch.max(torch.abs(abs_y[j][key] - medians[key])).item()\n",
    "                    if diff > max_diff:\n",
    "                        max_diff = diff\n",
    "                delta_ys.append(max_diff)\n",
    "\n",
    "            # 3. Compute MAD and threshold\n",
    "            median_delta = np.median(delta_ys)\n",
    "            deviations = np.abs(delta_ys - median_delta)\n",
    "            MAD_val = np.median(deviations)\n",
    "            threshold = rejection_threshold * MAD_val\n",
    "            epsilon = 1e-2  \n",
    "            threshold = max(threshold, epsilon) # TO avaoid zero threshold\n",
    "\n",
    "            # 4. Update suspicion scores\n",
    "            for idx, j in enumerate(neighbors):\n",
    "                # if j in lying_nodes:\n",
    "                # print(f\"Node {i} check {j}: ΔY={delta_ys[idx]:.2f}, threshold={threshold:.2f}\")\n",
    "                if delta_ys[idx] > threshold:\n",
    "                    print(f\"To {i} Value of {j}: ΔY={delta_ys[idx]:.2f}, threshold={threshold:.2f}, D = {D[i][j]}\")\n",
    "                # if j in lying_nodes:\n",
    "                    D[i][j] += 1\n",
    "                    # if Print_Val:\n",
    "                    # print(f\"Node {i} suspicious of {j}: ΔY={delta_ys[idx]:.2f}, threshold={threshold:.2f}, D = {D[i][j]}\")\n",
    "\n",
    "            # 5. Periodic mitigation check\n",
    "            if count % K_decision == 0 and count > 0:\n",
    "                for j in list(D[i].keys()):  # Iterate over copy to allow modification\n",
    "                    if D[i][j] > K_decision/2:\n",
    "                        # print(f\"Node {i} ignoring node {j} for next {K_decision} iterations\")\n",
    "                        mask[i][j] = -1\n",
    "                    else:\n",
    "                        mask[i][j] = 1\n",
    "                    D[i][j] = 0\n",
    "        mask_history.append(mask.copy())\n",
    "\n",
    "        # Next, update z values based on received y_transmit (unmask for SMPC)\n",
    "        if PrivacyMethod == 2:\n",
    "            for i in range(num_nodes):\n",
    "                for j in G.neighbors(i):\n",
    "                    if mask[i][j] == -1:\n",
    "                        for key in True_avg_dict:\n",
    "                            z[i][j][key] = (1 - averaging) * z[i][j][key]\n",
    "                    if mask[i][j] == 1:\n",
    "                        for key in True_avg_dict:\n",
    "                            # Receive from neighbor j to i: unmask the value\n",
    "                            if j < i:\n",
    "                                unmasked = y_transmit[j][i][key] + smpc_masks[(j, i)][key]\n",
    "                            else:\n",
    "                                unmasked = y_transmit[j][i][key] - smpc_masks[(i, j)][key]\n",
    "                            z[i][j][key] = (1 - averaging) * z[i][j][key] + (averaging) * unmasked\n",
    "        else:\n",
    "            for i in range(num_nodes):\n",
    "                for j in G.neighbors(i):\n",
    "                    if mask[i][j] == -1:\n",
    "                        for key in True_avg_dict:\n",
    "                            z[i][j][key] = (1 - averaging) * z[i][j][key]\n",
    "                    if mask[i][j] == 1:\n",
    "                        for key in True_avg_dict:\n",
    "                            z[i][j][key] = (1 - averaging) * z[i][j][key] + (averaging) * y_transmit[j][i][key].clone()\n",
    "\n",
    "\n",
    "        # --------------------\n",
    "        # 4. Synchronous z-update with masking\n",
    "        # --------------------\n",
    "        for i in range(num_nodes):\n",
    "            for j in G.neighbors(i):\n",
    "                if mask[i][j] == -1:\n",
    "                    # Block transmissions for each parameter in y[i][j]\n",
    "                    for key in True_avg_dict:\n",
    "                        # Just set it to zero (same shape)\n",
    "                        y_transmit[i][j][key] = torch.zeros_like(y[i][j][key])\n",
    "                else:\n",
    "                    # Pass the full dictionary\n",
    "                    for key in True_avg_dict:\n",
    "                        y_transmit[i][j][key] = y[i][j][key].clone()\n",
    "\n",
    "        for i in range(num_nodes):\n",
    "            for j in G.neighbors(i):\n",
    "                if mask[i][j] == -1:\n",
    "                    # Apply noise to blocked channels\n",
    "                    for key in True_avg_dict:\n",
    "                        z[i][j][key] = (1 - averaging) * z[i][j][key] \n",
    "                        # if torch.norm(z[i][j][key]) == 0:\n",
    "                        #     print(\"Pizza\")\n",
    "                if mask[i][j] == 1:\n",
    "                    # Normal update from y values\n",
    "                    for key in True_avg_dict:\n",
    "                        z[i][j][key] = (1 - averaging) * z[i][j][key]  + (averaging) * y_transmit[j][i][key].clone()\n",
    "\n",
    "        # --------------------\n",
    "        # 5. Update global model and check convergence\n",
    "        # --------------------\n",
    "        # Initialize global dictionary as zero tensors of the same shape\n",
    "        # for key in True_avg_dict.keys():\n",
    "        #     True_avg_dict[key] = sum(x[i][key] for i in true_nodes) / len(true_nodes)\n",
    "        # total_elements = 0\n",
    "        # param_keys = list(True_avg.state_dict().keys())  # Get the list of parameter keys\n",
    "\n",
    "        avg_error = 0\n",
    "        total_elements = 0\n",
    "\n",
    "        for i in true_nodes:\n",
    "            node_error = 0\n",
    "            for key in True_avg_dict:\n",
    "                diff = x[i][key] - True_avg_dict[key]\n",
    "                norm_diff = torch.norm(diff).item()\n",
    "                node_error += norm_diff\n",
    "            avg_error += node_error\n",
    "            total_elements += 1\n",
    "\n",
    "        # Final norm: average error divided by the total number of nodes\n",
    "        avg_error /= total_elements\n",
    "\n",
    "        # Store the computed average error\n",
    "        Error.append(avg_error)\n",
    "        print(f\"cunt {count} and error {Error[-1]}\" )\n",
    "\n",
    "        if avg_error < tolerance:\n",
    "            print(f'Converged at iteration {count}')\n",
    "            converged = True\n",
    "        elif count % 10 == 0 and Print_Val:\n",
    "            print(f'Iter {count}: Error {avg_error:.4f}')\n",
    "\n",
    "        # Update parameters for next iteration\n",
    "        count += 1\n",
    "    # Update final models\n",
    "    for i in range(num_nodes):\n",
    "        model_dict = node_models[i].state_dict()\n",
    "        for key in True_avg_dict:\n",
    "            model_dict[key] = x[i][key].clone()\n",
    "        node_models[i].load_state_dict(model_dict)\n",
    "\n",
    "    # Create an averaged model with the final True_avg_dict\n",
    "    averaged_model = type(node_models[0])(input_size, hidden_size, num_classes).to(device)  # Pass necessary arguments directly\n",
    "    averaged_model.load_state_dict(True_avg_dict)\n",
    "\n",
    "\n",
    "    return node_models, Error, mask_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(models, test_dataset, criterion):\n",
    "    \"\"\"Evaluate either a single model or list of models\"\"\"\n",
    "\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    if not isinstance(models, (list, tuple)):\n",
    "        models = [models]\n",
    "    avg_loss = 0.0\n",
    "    avg_accuracy = 0.0\n",
    "    \n",
    "    for model in models:\n",
    "        model.to(device)  # Move model to the correct device\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            running_loss = 0.0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            avg_loss += running_loss / len(test_loader)\n",
    "            avg_accuracy += 100 * correct / total\n",
    "\n",
    "    # Average across all models\n",
    "    avg_loss /= len(models)\n",
    "    avg_accuracy /= len(models)\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                      \n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Normalize the pixel values \n",
    "])\n",
    "\n",
    "\n",
    "# Download the FashionMNIST dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, transform=transform, download=True\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "\n",
    "class FlipLabelDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.label_map = {\n",
    "            0: 3,\n",
    "            1: 4,\n",
    "            2: 7,\n",
    "            3: 5,\n",
    "            4: 8,\n",
    "            5: 0,\n",
    "            6: 9,\n",
    "            7: 6,\n",
    "            8: 2,\n",
    "            9: 1\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        flipped_label = self.label_map[label]\n",
    "        return image, flipped_label\n",
    "\n",
    "# Split the training dataset into subsets for each client\n",
    "client_datasets = split_dataset(train_dataset, num_clients)\n",
    "\n",
    "# Flip labels for corrupt clients (AFTER splitting)\n",
    "for client_idx in lying_nodes:\n",
    "    print(f\"fliiping labels for client {client_idx}\")\n",
    "    original_dataset = client_datasets[client_idx]\n",
    "    if typeAttack != 6:\n",
    "        client_datasets[client_idx] = FlipLabelDataset(original_dataset)\n",
    "\n",
    "local_intialized_models = [None] * num_clients\n",
    "\n",
    "for client_idx in range(num_clients):\n",
    "    # Initialize the  model\n",
    "    local_intialized_models[client_idx] = Current(input_size, hidden_size, num_classes).to(device)\n",
    "    # local_models2 = MLP(input_size, hidden_size, num_classes).to(device)\n",
    "    local_intialized_models[client_idx].train()\n",
    "\n",
    "# Loss and optimizer (used for evaluation on the test set)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(local_models)\n",
    "# for client_idx in range(num_clients):\n",
    "#     print(f'\\nClient {client_idx+1}/{num_clients} training on local data...')\n",
    "#     local_models[client_idx]= train_local_model(client_datasets[client_idx], local_models[client_idx], num_epochs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# BlaBla = copy.deepcopy(local_models)\n",
    "# local_models22 = BlaBla.copy()\n",
    "\n",
    "# local_models22 = CorruptData(CorruptClients, local_models22, typeAttack, var, mean, Target,num_clients,scale)\n",
    "# global_model2 = aggregate_models2(local_models22, input_size, hidden_size, num_classes, true_nodes)\n",
    "# global_model22 = aggregate_models2(local_models22, input_size, hidden_size, num_classes, all_nodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejection_threshold = 99**9\n",
    "# K_decision = 1\n",
    "# when = -1\n",
    "\n",
    "# local_models2, PDMM_error, mask_history = aggregate_models(global_model2, local_models22, G, tolerance, c, max_iters, rejection_threshold, K_decision, averaging, when, CorruptValue, true_nodes,True, noise_STD)\n",
    "\n",
    "# honest_models = [local_models2[i] for i in true_nodes]\n",
    "# # Evaluate the global model on the test dataset\n",
    "# test_loss1, test_accuracy1 = evaluate(honest_models, test_dataset, criterion)\n",
    "# test_loss2, test_accuracy2 = evaluate(global_model2, test_dataset, criterion)\n",
    "# test_loss3, test_accuracy3 = evaluate(global_model22, test_dataset, criterion)\n",
    "\n",
    "# print(f'\\nAfter Round  Average Test Loss: {test_loss1:.4f}, Average Test Accuracy: {test_accuracy1:.2f}% for PDMM Average')\n",
    "# print(f'\\nAfter Round  Average Test Loss: {test_loss2:.4f}, Average Test Accuracy: {test_accuracy2:.2f}% for True Average')\n",
    "# print(f'\\nAfter Round  Average Test Loss: {test_loss3:.4f}, Average Test Accuracy: {test_accuracy3:.2f}% for Incorrect True Average')\n",
    "\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# # Plot accuracy\n",
    "# plt.plot(range(len(PDMM_error)), PDMM_error, label='PDMM convergenc')\n",
    "# plt.yscale('log')\n",
    "# plt.title('PDMM erorr over tranmission rounds')\n",
    "# plt.xlabel('Transmission Round')\n",
    "# plt.ylabel('Error (%)')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to keep track of metrics\n",
    "if save == True:\n",
    "    log_filename = f\"results_log_typeAttack{typeAttack}_PrivacyMethod{PrivacyMethod}_Rounds{num_rounds}.txt\"\n",
    "\n",
    "results = [[] for _ in range(num_rounds)]\n",
    "\n",
    "if PrivacyMethod != 2:\n",
    "    for noise_STD in noise_levels:\n",
    "        global_train_losses = []\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "        PDMM_error = []\n",
    "        test_losses2 = []\n",
    "        test_accuracies2 = []\n",
    "        test_losses3 = []\n",
    "        test_accuracies3 = []\n",
    "        local_models = copy.deepcopy(local_intialized_models)\n",
    "        print(f\"\\n--- Doing noise level {noise_STD} with {num_rounds} rounds---\")\n",
    "        for round in range(num_rounds):\n",
    "            print(f'\\n--- Round {round+1} of {num_rounds} ---')\n",
    "            \n",
    "            # Each client trains on its local data\n",
    "            # local_models = [None] * num_clients\n",
    "\n",
    "            # print(local_models)\n",
    "            for client_idx in range(num_clients):\n",
    "                print(f'\\nClient {client_idx+1}/{num_clients} training on local data...')\n",
    "                local_models[client_idx]= train_local_model(client_datasets[client_idx], local_models[client_idx], num_epochs)\n",
    "\n",
    "            Local_models_trained = copy.deepcopy(local_models)\n",
    "            Local_models_trained = local_models.copy()\n",
    "\n",
    "            Local_models_trained = CorruptData(CorruptClients, Local_models_trained, typeAttack, var, mean, Target,num_clients,scale)\n",
    "            # print(global_model)\n",
    "            global_model2 = aggregate_models2(Local_models_trained, input_size, hidden_size, num_classes, true_nodes)\n",
    "            global_model22 = aggregate_models2(Local_models_trained, input_size, hidden_size, num_classes, all_nodes)\n",
    "\n",
    "            Local_models_trained, PDMM_error, mask_history = aggregate_models(global_model2, Local_models_trained, G, tolerance, c, max_iters, rejection_threshold, K_decision, averaging, when, CorruptValue, true_nodes, False, noise_STD, PrivacyMethod, 0, learning_rate)\n",
    "            # print(global_model)\n",
    "\n",
    "            # Calculate edges for metrics\n",
    "            edges = list(G.edges())\n",
    "            honest_nodes = np.where(CorruptClients == 0)[0]\n",
    "            lying_nodes = np.where(CorruptClients == 1)[0]\n",
    "\n",
    "            # Identify target edges\n",
    "            edges_honest_target = [(i, j) for (i, j) in edges if j in honest_nodes]\n",
    "            edges_corrupt_target = [(i, j) for (i, j) in edges if j in lying_nodes]\n",
    "\n",
    "            # Calculate total edges for normalization\n",
    "            total_honest_edges = len(edges_honest_target)\n",
    "            total_corrupt_edges = len(edges_corrupt_target)\n",
    "\n",
    "            # Initialize metrics\n",
    "            FAR_list = []\n",
    "            MDR_list = []\n",
    "\n",
    "            # Calculate metrics for each iteration\n",
    "            for step in range(len(mask_history)):\n",
    "                mask = mask_history[step]\n",
    "                \n",
    "                # False Alarm Rate (honest edges rejected)\n",
    "                false_alarms = sum(1 for (i, j) in edges_honest_target if mask[i, j] == -1)\n",
    "                far = false_alarms / total_honest_edges if total_honest_edges > 0 else 0\n",
    "                FAR_list.append(far)\n",
    "                \n",
    "                # Missed Detection Rate (corrupt edges not rejected)\n",
    "                missed_detections = sum(1 for (i, j) in edges_corrupt_target if mask[i, j] != -1)\n",
    "                mdr = missed_detections / total_corrupt_edges if total_corrupt_edges > 0 else 0\n",
    "                MDR_list.append(mdr)\n",
    "                \n",
    "            results[round].append({\n",
    "                'noise_STD': noise_STD**2,\n",
    "                'FAR': FAR_list,\n",
    "                'MDR': MDR_list,\n",
    "                'Error': PDMM_error\n",
    "                })\n",
    "            \n",
    "            honest_models = [Local_models_trained[i] for i in true_nodes]\n",
    "            # Evaluate the global model on the test dataset\n",
    "            test_loss, test_accuracy = evaluate(honest_models, test_dataset, criterion)\n",
    "            test_losses.append(test_loss)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "\n",
    "            # Evaluate the global model on the test dataset\n",
    "            test_loss2, test_accuracy2 = evaluate(global_model2, test_dataset, criterion)\n",
    "            test_losses2.append(test_loss2)\n",
    "            test_accuracies2.append(test_accuracy2)\n",
    "\n",
    "            # Evaluate the incorrect global model on the test dataset\n",
    "            test_loss3, test_accuracy3 = evaluate(global_model22, test_dataset, criterion)\n",
    "            test_losses3.append(test_loss3)\n",
    "            test_accuracies3.append(test_accuracy3)\n",
    "\n",
    "\n",
    "            print(f'\\nAfter Round {round+1}: Average Test Loss: {test_loss:.4f}, Average Test Accuracy: {test_accuracy:.2f}% for PDMM Average')\n",
    "\n",
    "            print(f'\\nAfter Round {round+1}: Test Loss: {test_loss2:.4f}, Test Accuracy: {test_accuracy2:.2f}% for True Average')\n",
    "\n",
    "            print(f'\\nAfter Round {round+1}: Test Loss: {test_loss3:.4f}, Test Accuracy: {test_accuracy3:.2f}% for Incorrect True Average')\n",
    "            if save == True:\n",
    "                with open(log_filename, \"a\") as logfile:\n",
    "                    logfile.write(f\"After Round {round+1}:\\n\")\n",
    "                    logfile.write(f\"For noise {noise_STD}:\\n\")\n",
    "                    logfile.write(f\"  PDMM Average -> Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\\n\")\n",
    "                    logfile.write(f\"  True Average -> Test Loss: {test_loss2:.4f}, Test Accuracy: {test_accuracy2:.2f}%\\n\")\n",
    "                    logfile.write(f\"  Incorrect True Average -> Test Loss: {test_loss3:.4f}, Test Accuracy: {test_accuracy3:.2f}%\\n\\n\")\n",
    "            \n",
    "\n",
    "            # Check stopping criteria\n",
    "            if test_loss <= threshold:\n",
    "                print(f'\\nLoss threshold reached. Stopping training.')\n",
    "                break\n",
    "            # Plot test accuracy and loss over rounds\n",
    "        plt.figure(figsize=(10, 4))\n",
    "\n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(num_rounds), test_accuracies, 'ro-', label='Test Accuracy')\n",
    "        plt.title('Test Accuracy over Rounds for PDMM Average')\n",
    "        plt.xlabel('Round')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(num_rounds), test_losses, 'bo-', label='Test Loss')\n",
    "        plt.title('Test Loss over Rounds for PDMM Average')\n",
    "        plt.xlabel('Round')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        filename_PDMM = f\"PDMM_Average_typeAttack{typeAttack}_PrivacyMethod{PrivacyMethod}_noise{noise_STD}_Rounds{num_rounds}.png\"\n",
    "        plt.savefig(filename_PDMM, dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "        # Plot test accuracy and loss over rounds\n",
    "        plt.figure(figsize=(10, 4))\n",
    "\n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(num_rounds), test_accuracies2, 'ro-', label='Test Accuracy')\n",
    "        plt.title('Test Accuracy over Rounds for True Average')\n",
    "        plt.xlabel('Round')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(num_rounds), test_losses2, 'bo-', label='Test Loss')\n",
    "        plt.title('Test Loss over Rounds for True Average')\n",
    "        plt.xlabel('Round')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        filename_True = f\"True_Average_typeAttack{typeAttack}_PrivacyMethod{PrivacyMethod}_noise{noise_STD}_Rounds{num_rounds}.png\"\n",
    "        plt.savefig(filename_True, dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "if PrivacyMethod == 2:\n",
    "    for p in PrimModulo:\n",
    "        global_train_losses = []\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "        PDMM_error = []\n",
    "        test_losses2 = []\n",
    "        test_accuracies2 = []\n",
    "        test_losses3 = []\n",
    "        test_accuracies3 = []\n",
    "        local_models = copy.deepcopy(local_intialized_models)\n",
    "        print(f\"\\n--- Doing SMPC with {num_rounds} rounds---\")\n",
    "        for round in range(num_rounds):\n",
    "            print(f'\\n--- Round {round+1} of {num_rounds} ---')\n",
    "            \n",
    "            # Each client trains on its local data\n",
    "            # local_models = [None] * num_clients\n",
    "\n",
    "            # print(local_models)\n",
    "            for client_idx in range(num_clients):\n",
    "                print(f'\\nClient {client_idx+1}/{num_clients} training on local data...')\n",
    "                local_models[client_idx]= train_local_model(client_datasets[client_idx], local_models[client_idx], num_epochs)\n",
    "\n",
    "            Local_models_trained = copy.deepcopy(local_models)\n",
    "            Local_models_trained = local_models.copy()\n",
    "\n",
    "            Local_models_trained = CorruptData(CorruptClients, Local_models_trained, typeAttack, var, mean, Target,num_clients,scale)\n",
    "            # print(global_model)\n",
    "            global_model2 = aggregate_models2(Local_models_trained, input_size, hidden_size, num_classes, true_nodes)\n",
    "            global_model22 = aggregate_models2(Local_models_trained, input_size, hidden_size, num_classes, all_nodes)\n",
    "\n",
    "            Local_models_trained, PDMM_error, mask_history = aggregate_models(global_model2, Local_models_trained, G, tolerance, c, max_iters, rejection_threshold, K_decision, averaging, when, CorruptValue, true_nodes, False, 0, PrivacyMethod, p, learning_rate)\n",
    "            # print(global_model)\n",
    "\n",
    "            # Calculate edges for metrics\n",
    "            edges = list(G.edges())\n",
    "            honest_nodes = np.where(CorruptClients == 0)[0]\n",
    "            lying_nodes = np.where(CorruptClients == 1)[0]\n",
    "\n",
    "            # Identify target edges\n",
    "            edges_honest_target = [(i, j) for (i, j) in edges if j in honest_nodes]\n",
    "            edges_corrupt_target = [(i, j) for (i, j) in edges if j in lying_nodes]\n",
    "\n",
    "            # Calculate total edges for normalization\n",
    "            total_honest_edges = len(edges_honest_target)\n",
    "            total_corrupt_edges = len(edges_corrupt_target)\n",
    "\n",
    "            # Initialize metrics\n",
    "            FAR_list = []\n",
    "            MDR_list = []\n",
    "\n",
    "            # Calculate metrics for each iteration\n",
    "            for step in range(len(mask_history)):\n",
    "                mask = mask_history[step]\n",
    "                \n",
    "                # False Alarm Rate (honest edges rejected)\n",
    "                false_alarms = sum(1 for (i, j) in edges_honest_target if mask[i, j] == -1)\n",
    "                far = false_alarms / total_honest_edges if total_honest_edges > 0 else 0\n",
    "                FAR_list.append(far)\n",
    "                \n",
    "                # Missed Detection Rate (corrupt edges not rejected)\n",
    "                missed_detections = sum(1 for (i, j) in edges_corrupt_target if mask[i, j] != -1)\n",
    "                mdr = missed_detections / total_corrupt_edges if total_corrupt_edges > 0 else 0\n",
    "                MDR_list.append(mdr)\n",
    "                \n",
    "            results[round].append({\n",
    "                'FAR': FAR_list,\n",
    "                'MDR': MDR_list,\n",
    "                'Error': PDMM_error\n",
    "                })\n",
    "            \n",
    "            honest_models = [Local_models_trained[i] for i in true_nodes]\n",
    "            # Evaluate the global model on the test dataset\n",
    "            test_loss, test_accuracy = evaluate(honest_models, test_dataset, criterion)\n",
    "            test_losses.append(test_loss)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "\n",
    "            # Evaluate the global model on the test dataset\n",
    "            test_loss2, test_accuracy2 = evaluate(global_model2, test_dataset, criterion)\n",
    "            test_losses2.append(test_loss2)\n",
    "            test_accuracies2.append(test_accuracy2)\n",
    "\n",
    "            # Evaluate the incorrect global model on the test dataset\n",
    "            test_loss3, test_accuracy3 = evaluate(global_model22, test_dataset, criterion)\n",
    "            test_losses3.append(test_loss3)\n",
    "            test_accuracies3.append(test_accuracy3)\n",
    "\n",
    "\n",
    "            print(f'\\nAfter Round {round+1}: Average Test Loss: {test_loss:.4f}, Average Test Accuracy: {test_accuracy:.2f}% for PDMM Average')\n",
    "\n",
    "            print(f'\\nAfter Round {round+1}: Test Loss: {test_loss2:.4f}, Test Accuracy: {test_accuracy2:.2f}% for True Average')\n",
    "\n",
    "            print(f'\\nAfter Round {round+1}: Test Loss: {test_loss3:.4f}, Test Accuracy: {test_accuracy3:.2f}% for Incorrect True Average')\n",
    "            if save == True:\n",
    "                with open(log_filename, \"a\") as logfile:\n",
    "                    logfile.write(f\"After Round {round+1}:\\n\")\n",
    "                    logfile.write(f\"For SMPC:\\n\")\n",
    "                    logfile.write(f\"  PDMM Average -> Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\\n\")\n",
    "                    logfile.write(f\"  True Average -> Test Loss: {test_loss2:.4f}, Test Accuracy: {test_accuracy2:.2f}%\\n\")\n",
    "                    logfile.write(f\"  Incorrect True Average -> Test Loss: {test_loss3:.4f}, Test Accuracy: {test_accuracy3:.2f}%\\n\\n\")\n",
    "            \n",
    "\n",
    "            # Check stopping criteria\n",
    "            if test_loss <= threshold:\n",
    "                print(f'\\nLoss threshold reached. Stopping training.')\n",
    "                break\n",
    "            # Plot test accuracy and loss over rounds\n",
    "        plt.figure(figsize=(10, 4))\n",
    "\n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(num_rounds), test_accuracies, 'ro-', label='Test Accuracy')\n",
    "        plt.title('Test Accuracy over Rounds for PDMM Average')\n",
    "        plt.xlabel('Round')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(num_rounds), test_losses, 'bo-', label='Test Loss')\n",
    "        plt.title('Test Loss over Rounds for PDMM Average')\n",
    "        plt.xlabel('Round')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        filename_PDMM = f\"PDMM_Average_typeAttack{typeAttack}_PrivacyMethod{PrivacyMethod}_SMPC_Rounds{num_rounds}.png\"\n",
    "        plt.savefig(filename_PDMM, dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "        # Plot test accuracy and loss over rounds\n",
    "        plt.figure(figsize=(10, 4))\n",
    "\n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(num_rounds), test_accuracies2, 'ro-', label='Test Accuracy')\n",
    "        plt.title('Test Accuracy over Rounds for True Average')\n",
    "        plt.xlabel('Round')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(num_rounds), test_losses2, 'bo-', label='Test Loss')\n",
    "        plt.title('Test Loss over Rounds for True Average')\n",
    "        plt.xlabel('Round')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        filename_True = f\"True_Average_typeAttack{typeAttack}_PrivacyMethod{PrivacyMethod}_SMPC_Rounds{num_rounds}.png\"\n",
    "        plt.savefig(filename_True, dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for round in range(num_rounds):\n",
    "    plot_combined_metrics(results[round], round, typeAttack, PrivacyMethod, save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
